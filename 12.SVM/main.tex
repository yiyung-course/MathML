%\pdfminorversion=4
\documentclass[handout,fleqn,aspectratio=169]{beamer}

\input{../myhead}

\title[]{Lecture 12: Classification with Support Vector Machines}
\author{Yi, Yung (이융)}
\institute{Mathematics for Machine Learning\\ \url{https://yung-web.github.io/home/courses/mathml.html}
\\KAIST EE}
\date{\today}


\input{../mymath}
\input{../mymacro}


%\addtobeamertemplate{footline}{\rule{0.94\paperwidth}{1pt}}{}

\begin{document}

\input{../mydefault}



% START START START START START START START START START START START START START

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Warm-Up}

{\Large Please watch this tutorial video by Luis Serrano on Support Vector Machine.}

\bigskip

\bigskip

\url{https://youtu.be/Lpr__X8zuE8}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item Story and Separating Hyperplanes 
\item Primal SVM: Hard SVM 
\item Primal SVM: Soft SVM 
\item Dual SVM 
\item Kernels 
\item Numerical Solution 

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L12(1)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \redf{Story and Separating Hyperplanes}
\item \grayf{Primal SVM: Hard SVM 
\item Primal SVM: Soft SVM 
\item Dual SVM 
\item Kernels 
\item Numerical Solution}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Storyline}

\plitemsep 0.1in

\bci

\item (Binary) classification vs. regression

\item A Classification predictor $f:\realD \mapsto \{+1, -1 \},$ where $D$ is the dimension of features.
\item Suppervised learning as in the regression with a given dataset $\{(\vx_1,y_1), \ldots, (\vx_N,y_N) \},$ where our task is to learn the model parameters which produces the smallest classification errors. 

\item SVM
\bci
\item Geometric way of thinking about supvervised learning
\item Relying on empirical risk minimization
\item Binary classification = Drawing a separating hyperplane
\item Various interpretation from various perspectives: geometric view, loss function view, the view from convex hulls of data points 
\eci
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hard SVM vs. Soft SVM}

\mypic{0.55}{L12_soft_hard_svm.png}

\plitemsep 0.1in

\bci

\item Hard SVM: Linearly separable, and thus, allow  no classification error 

\item Soft SVM: Non-linearly separable, thus, allow some classification error
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Separating Hyperplane}

\plitemsep 0.07in

\bci

\item \bluef{Hyperplane} in $\realD$ is a set:
$\{x \mid \trans{a}x=b\}$ where $a\in\realn, a\neq 0, b\in\real$ \hfill \lecturemark{L7(3)}

In other words, $\{ x \mid \trans{a}(x-x_0) =0\},$ where $x_0$ is any point in
the hyperplane, i.e., $\trans{a} x_0 = b.$

\mysmalltwocols{0.2}
{
\item Divides $\realD$ into two {\blue halfspaces}: 
$\{x|\trans{a}x\leq b\}$ and $\{x|\trans{a}x>b\}$
}
{
\vspace{-0.3cm}
\mypic{0.7}{L12_halfspace.png}
}
\vspace{-0.2cm}
\item In our problem, we consider the hyperplane $\trans{\vw}\vx + b=0,$ where $\vw$ and $b$ are the parameters of the model.

\item Classification logic
\aleq{
\begin{cases}
\trans{\vw}\vx_n + b \geq 0 & \ \text{when} \ y_n = +1\cr
\trans{\vw}\vx_n + b < 0 & \ \text{when} \ y_n = -1
\end{cases}
\implies \redf{y_n \big(\trans{\vw}\vx_n +b \big) \geq 0}
}

% \bci
% \item $\trans{\vw}\vx_n + b \geq 0$ when $y_n = +1$
% \item $\trans{\vw}\vx_n + b < 0$ when $y_n = -1$
% \eci
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Distance bertween Two Hyperplanes}

\plitemsep 0.07in

\bci

\item Consider two hyperplanes $\trans{\vw}\vx - b =0$ and $\trans{\vw}\vx - b= r$, where assume $r >0.$

\item \question What is the distance\footnote{Shortested distance between two hyperplanes.} between two hyperplanes? Answer: \bluef{$\dfrac{r}{\norm{w}}$}
\eci

\vspace{-0.7cm}
\mypic{0.5}{L12_disthyper.png}

% \mysmalltwocols{0.4}
% {
% }
% {

% }


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L12(2)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Story and Separating Hyperplanes}
\item \redf{Primal SVM: Hard SVM} 
\item \grayf{Primal SVM: Soft SVM 
\item Dual SVM 
\item Kernels 
\item Numerical Solution} 

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hard Support Vector Machine}

\plitemsep 0.07in

\bci

\item Assume that the data points are linearly separable.

\item Goal: Find the hyperplane that maximizes the margin between the positive and the negative samples

\item Given the training dataset $\{(\vx_1,y_1), \ldots, (\vx_N,y_N) \}$ 
and a hyperplane $\trans{\vw}\vx + b =0,$ what is the constraint that all data points are $\frac{r}{\norm{w}}$-away from the hyperplane?
$$
y_n \big(\trans{\vw}\vx_n +b \big) \geq \frac{r}{\norm{\vw}}
$$

\item Note that $r$ and $\norm{w}$ are scaled together, so if we fix $\norm{w}=1$, then 
$$
y_n \big(\trans{\vw}\vx_n +b \big) \geq r
$$

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hard SVM: Formulation 1}

\plitemsep 0.07in

\bci

\item Maximize the margin, such that all the training data points are well-classified into their classes ($+$ or $-$)
\mycolorbox
{
\vspace{-0.3cm}
\aleq{
\max_{\vw, b, r} \quad &r \cr
\text{subject to} \quad & y_n \big(\trans{\vw}\vx_n +b \big) \geq r, \ \text{for all} \ n=1,\ldots, N, \quad \norm{\vw}=1, \quad r>0
}
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Formulation 2 (1)}

\mycolorbox
{
\aleq{
\max_{\vw, b, r} \quad &r \cr
\text{subject to} \quad & y_n \big(\trans{\vw}\vx_n +b \big) \geq r, \ \text{for all} \ n=1,\ldots, N, \quad \norm{\vw}=1, \quad r>0
}
}
\plitemsep 0.07in
\bci

\item Since $\norm{\vw}=1,$ reformulate $\vw$ by $\vw'$ as: 
$y_n \Big(\dfrac{\trans{\vw'}}{\norm{\vw'}}\vx_n +b \Big) \geq r$
\item Change the objective from $r$ to $r^2.$
\item Define $\vw''$ and $b''$ by rescaling the constraint:
\aleq{
y_n \Big(\frac{\trans{\vw'}}{\norm{\vw'}}\vx_n +b \Big) \geq r \Longleftrightarrow
y_n \Big(\trans{\vw''}\vx_n +b'' \Big) \geq 1, \quad 
\vw'' = \frac{\vw'}{\norm{\vw'}r} \ \text{and} \ b'' = \frac{b}{r}
}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Formulation 2 (2)}

\plitemsep 0.07in
\bci

\item Note that $\norm{\vw''} = \frac{1}{r}$
\item Thus, we have the following reformulated problem:
\mycolorbox
{
\vspace{-0.3cm}
\aleq{
\max_{\vw'', b''} \quad &\frac{1}{\norm{\vw''}^2} \cr
\text{subject to} \quad & y_n \big(\trans{\vw''}\vx_n +b'' \big) \geq 1, \ \text{for all} \ n=1,\ldots, N,
}
}
=

\mycolorbox
{
\vspace{-0.3cm}
\aleq{
\min_{\vw, b} \quad &\frac{1}{2} \norm{\vw}^2 \cr
\text{subject to} \quad & y_n \big(\trans{\vw}\vx_n +b \big) \geq 1, \ \text{for all} \ n=1,\ldots, N,
}
}


\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Understanding Formulation 2 Intuitively}

\plitemsep 0.07in
\bci

\item Given the training dataset $\{(\vx_1,y_1), \ldots, (\vx_N,y_N) \}$ 
and a hyperplane $\trans{\vw}\vx + b =0,$ what is the constraint that all data points are $\frac{r}{\norm{w}}$-away from the hyperplane?
$$
y_n \big(\trans{\vw}\vx_n +b \big) \geq \frac{r}{\norm{\vw}}
$$

\item \redf{Formulation 1.} Note that $r$ and $\norm{w}$ are scaled together, so if we fix $\norm{w}=1$, then 
$$
y_n \big(\trans{\vw}\vx_n +b \big) \geq r.
$$
And, \bluef{maximize $r.$}

\item \redf{Formulation 2.} If we fix $r=1,$ then  
$$
y_n \big(\trans{\vw}\vx_n +b \big) \geq 1.
$$
And, minimize $\norm{\vw}$
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L12(3)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Story and Separating Hyperplanes}
\item \grayf{Primal SVM: Hard SVM} 
\item \redf{Primal SVM: Soft SVM} 
\item \grayf{Dual SVM 
\item Kernels 
\item Numerical Solution} 

\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Soft SVM: Geometric View}

\plitemsep 0.07in
\bci

\item Now we allow some classification errors, because it's not linearly separable. 

\item Introduce a slack variable that quantifies how much errors will be allowed in my optimization problem
\mytwocols{0.6}
{
\small
\item $\vxi = (\xi_n: n=1, \ldots, N)$
\item $\xi_n$: slack for the $n$-th sample $(\vx_n,y_n)$
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\vspace{-0.3cm}
\aleq{
\min_{\vw, b} \quad &\frac{1}{2} \norm{\vw}^2 +C\sum_{n=1}^N \xi_n \cr
\text{subject to} \quad & y_n \big(\trans{\vw}\vx_n +b \big) \geq 1 - \xi_n,\cr
& \xi_n \geq 0, \qquad \text{for all} \ n
}
\end{tcolorbox}

\item $C$: Trade-off between width and slack
}
{
%\vspace{-0.4cm}
\mypic{0.75}{L12_softsvm_geo.png}
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Soft SVM: Loss Function View (1)}

\plitemsep 0.07in
\bci

\item From the perspective of empirical risk minimizaiton

\item Loss function design
\bci
\item \bluef{zero-one loss} $\mathbf{1}(f(x_n) \neq y_n)$: \# of mismatches between the prediction and the label $\implies$ combinatorial optimization (typically NP-hard)

\item \bluef{hinge loss}
$$
\ell(t) = \max(0,1-t), \ \text{where} \ t = y f(\vx) = y(\trans{\vw}\vx + b)
$$

\mysmalltwocols{0.4}
{
\bci
\item If $\vx$ is really at the correct side, $t \geq 1$ $\rightarrow$ $\ell(t) =0$
\item If $\vx$ is at the correct side, but too close to the boundary, $0 < t < 1$ \\$\rightarrow$ $0< \ell(t) =1-t <1$
\item If $\vx$ is at the wrong side, $ t < 0$ \\$\rightarrow$ $1 < \ell(t) =1-t$
\eci
}
{
\mypic{0.8}{L12_hingeloss.png}
}

\eci

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Soft SVM: Loss Function View (2)}

\mycolorbox{
\vspace{-0.3cm}
\aleq{
\min_{\vw, b} \ \text{(regularizer + loss)} = \min_{\vw, b} \quad \frac{1}{2} \norm{\vw}^2 +C\sum_{n=1}^N \max \{0,1- y(\trans{\vw}\vx + b) \}
}
}
\plitemsep 0.1in
\bci

\item $\frac{1}{2}\norm{\vw}^2$: L2-regularizer (margin maximization = regularization)

\item $C$: regularization parameter, which moves from the regularization term to the loss term
\item Why this loss function view = geometric view?
\aleq{
\min_t \max(0,1-t) \Longleftrightarrow \min_{\xi,t} \xi, \ \text{subject to} \ \xi \geq 0, \ \xi \geq 1-t
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L12(4)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Story and Separating Hyperplanes}
\item \grayf{Primal SVM: Hard SVM} 
\item \grayf{Primal SVM: Soft SVM} 
\item \red{Dual SVM} 
\item \grayf{Kernels 
\item Numerical Solution} 

\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Dual SVM: Idea}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\vspace{-0.3cm}
\aleq{
\min_{\vw, b} \quad &\frac{1}{2} \norm{\vw}^2 +C\sum_{n=1}^N \xi_n \cr
\text{subject to} \quad & y_n \big(\trans{\vw}\vx_n +b \big) \geq 1 - \xi_n, \ \xi_n \geq 0, \quad \text{for all} \ n
}
\end{tcolorbox}

\vspace{-0.3cm}
\plitemsep 0.05in
\bci

\item The above primal problem is a convex optimization problem. 

\item Let's apply Lagrange multipliers, find another formulation, and see what other nice properties are shown \hfill \lecturemark{L7(2), L7(4)}

\item Convert the problem into "$\leq$" constraints, so as to apply \redf{min-min-max} rule
\mycolorbox{
\vspace{-0.3cm}
\aleq{
\min_{\vw, b} \ \frac{1}{2} \norm{\vw}^2 +C\sum_{n=1}^N \xi_n, \  
\text{s.t.} \  -y_n \big(\trans{\vw}\vx_n +b \big) \leq -1 + \xi_n, \ -\xi_n \leq 0, \quad \text{for all} \ n
}
}

% \item Lagrangian
% \aleq{
% \cL(\vw, b, \vxi, \valpha, \vgamma) = \frac{1}{2} \norm{\vw}^2 +C\sum_{n=1}^N \xi_n
% - \sum_{n=1}^N \alpha_n\Big[y_n \big(\trans{\vw}\vx_n +b \big) -1 + \xi_n \Big] - \sum_{n=1}^N \gamma_n \xi_n
% }

\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Applying Lagrange Multipliers (1)}

\mycolorbox{
\vspace{-0.3cm}
\aleq{
\min_{\vw, b} \ \frac{1}{2} \norm{\vw}^2 +C\sum_{n=1}^N \xi_n, \  
\text{s.t.} \  -y_n \big(\trans{\vw}\vx_n +b \big) \leq -1 + \xi_n, \ -\xi_n \leq 0, \quad \text{for all} \ n
}
}
\vspace{-0.5cm}
\plitemsep 0.05in
\bci

\item Lagrangian with multipliers $\alpha_n \geq 0$ and $\gamma_n \geq 0$
\aleq{
\cL(\vw, b, \vxi, \valpha, \vgamma) = \frac{1}{2} \norm{\vw}^2 +C\sum_{n=1}^N \xi_n
- \sum_{n=1}^N \alpha_n\Big[y_n \big(\trans{\vw}\vx_n +b \big) -1 + \xi_n \Big] - \sum_{n=1}^N \gamma_n \xi_n
}

\item Dual function: $\cD(\valpha,\vgamma) = \inf_{\vw, b, \vxi} \cL(\vw, b, \vxi, \valpha, \vgamma)$ for which the followings should be met:
\small
\aleq{
\text{\blue (D1)} \ \pd{\cL}{\vw} = \trans{\vw} - \sum_{n=1}^N \alpha_n y_n \trans{\vx}_n = 0, \ \text{\blue (D2)} \ \pd{\cL}{b} = \sum_{n=1}^N \alpha_n y_n =0 , \ \text{(\blue D3)} \ \pd{\cL}{\xi_n} = C - \alpha_n - \gamma_n = 0
}
\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Applying Lagrange Multipliers (2)}

\plitemsep 0.07in
\bci

\item Dual function $\cD(\valpha,\vgamma) = \inf_{\vw, b, \vxi} \cL(\vw, b, \vxi, \valpha, \vgamma)$ with \bluef{(D1)} is given by:
\aleq{
\cD(\valpha,\vgamma) &= \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j \alpha_i \alpha_j 
\inner{\vx_i}{\vx_j} - \redf{\sum_{i=1}^N y_i \alpha_i} \inner{\sum_{j=1}^N y_j \alpha_j \vx_j}{\vx_i} -b \redf{\sum_{i=1}^N y_i \alpha_i} \cr
&  + \sum_{i=1}^N \alpha_i + \sum_{i=1}^N \magenf{(C-\alpha_i -\gamma_i)}\xi_i
}

\item From \redf{(D2)} and \magenf{(D3)}, the above is simplified into:
\aleq{
\cD(\valpha,\vgamma) = \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j \alpha_i \alpha_j 
\inner{\vx_i}{\vx_j} + \sum_{i=1}^N \alpha_i 
}

\item $\alpha_i, \gamma_i \geq 0$ and $C-\alpha_i-\gamma_i =0$ $\implies$ $ 0 \le \alpha_i \le C$
\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Dual SVM}

\plitemsep 0.07in
\bci

\item (Lagrangian) Dual Problem: \redf{maximize $\cD(\valpha,\vgamma)$}
\mycolorbox
{
\vspace{-0.3cm}
\aleq{
\min_{\valpha} \quad & \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j \alpha_i \alpha_j 
\inner{\vx_i}{\vx_j} + \sum_{i=1}^N \alpha_i \cr
\text{subject to} \quad& \sum_{i=1}^N y_i \alpha_i =0, \quad 0 \le \alpha_i \le C, \ \forall i=1, \ldots, N
}
\vspace{-0.2cm}
}
\item Primal SVM: the number of parameters scales as \bluef{the number of features ($D$)}

\item Dual SVM
\bci
\item the number of parameters scales as \bluef{the number of training data ($N$)}
\item only depends on the inner products of individual training data points $\inner{\vx_i}{\vx_j}$ $\rightarrow$ allow the application of \redf{kernel}
\eci

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L12(5)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Story and Separating Hyperplanes}
\item \grayf{Primal SVM: Hard SVM} 
\item \grayf{Primal SVM: Soft SVM} 
\item \grayf{Dual SVM} 
\item \redf{Kernels 
\item Numerical Solution} 

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Kernel}

\mytwocols{0.7}
{
\bigskip

\plitemsep 0.1in
\bci

\item Modularity: Using the feature transformation $\vphi(\vx),$ dual SVMs can be modularized
$$
\inner{\vx_i}{\vx_j} \implies \inner{\vphi(\vx_i)}{\vphi(\vx_j)}
$$

\item Similarity function $k: \cX \times \cX \mapsto \real$, $k(\vx_i,\vx_j) = \inner{\vphi(\vx_i)}{\vphi(\vx_j)}$

\item Kernel matrix, Gram matrix: must be symmetric and positive semidifinite 

\item Examples: polynomial kernel, Gaussian radial basis function, rational quadratic kernel
\eci
}
{
\mypic{0.9}{L12_kernel_ex.png}
}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Numerical Solution}

\plitemsep 0.07in
\bci

\item 

\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\vspace{2cm}
\LARGE Questions?


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Review Questions}
% \tableofcontents
%\plitemsep 0.1in
\bce[1)]
\item 

\ece
\end{frame}


\end{document}

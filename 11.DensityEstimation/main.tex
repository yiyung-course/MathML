%\pdfminorversion=4
\documentclass[handout,fleqn,aspectratio=169]{beamer}

\input{../myhead}

\title[]{Lecture 11: Density Estimation \\with Gaussian Mixture Models}
\author{Yi, Yung (이융)}
\institute{Mathematics for Machine Learning\\ \url{https://yung-web.github.io/home/courses/mathml.html}
\\KAIST EE}
\date{\today}


\input{../mymath}
\input{../mymacro}


%\addtobeamertemplate{footline}{\rule{0.94\paperwidth}{1pt}}{}

\begin{document}

\input{../mydefault}



% START START START START START START START START START START START START START

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Warm-Up}

{\Large Please watch this tutorial video by Luis Serrano on Gaussian Mixture Model.}

\bigskip

\bigskip

\url{https://www.youtube.com/watch?v=q71Niz856KE}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item Gaussian Mixture Model
\item Parameter Learning: MLE
\item Latent-Variable Perspective for Probabilistic Modeling
\item EM Algorithm
\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L11(1)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \redf{Gaussian Mixture Model}
\item \grayf{Parameter Learning: MLE
\item Latent-Variable Perspective for Probabilistic Modeling
\item EM Algorithm}
\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Density Estimation}

\plitemsep 0.1in

\bci

\item Represent data compactly using a density from a parametric family, e.g., Gaussian or Beta distribution

\item Parameters of those families can be found by MLE and MAPE

\item However, there are many cases when simple distributions (e.g., just Gaussian) fail to approximate data.

\mypic{0.4}{L11_Gaussian_fail.png}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Mixture Models}

\plitemsep 0.1in

\bci

\item More expressive family of distribution

\item Idea: Let's mix! A \bluef{convex combination} of $K$ ``base'' distributions
\mycolorbox{
\vspace{-0.2cm}
\aleq{
p(\vx) = \sum_{k=1}^K \pi_k p_k(\vx), \quad 0 \le \pi_k \le 1, \quad \sum_{k=1}^K \pi_k = 1}
}
\item Multi-modal distributions: Can be used to describe datasets with multiple clusters

\item Our focus: Gaussian mixture models

\item Want to finding the parameters using MLE, but \bluef{cannot have the closed form} solution (even with the mixture of Gaussians) $\rightarrow$ some iterative methods needed
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Gaussian Mixture Model}

\mycolorbox{
\vspace{-0.2cm}
\aleq{
p(\vx | \vth) = \sum_{k=1}^K \set{N}(\vx | \vmu_k, \msig_k), \quad 0 \le \pi_k \le 1, \quad \sum_{k=1}^K \pi_k = 1,
}
where the parameters $\vth \eqdef \{\vmu_k, \msig_k, \pi_k: k= 1, \ldots, K \}$
}


\vspace{-0.3cm}
\plitemsep 0.01in

\bci
\item \exam $p(x|\vth) = \bluef{0.5\set{N}(x|-2,1/2)} + \orangef{0.2\set{N}(x|1,2)} + \greenf{0.3\set{N}(x|4,1)}$

\mypic{0.4}{L11_gm_ex.png}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L11(2)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Gaussian Mixture Model}
\item \redf{Parameter Learning: MLE}
\item \grayf{Latent-Variable Perspective for Probabilistic Modeling
\item EM Algorithm}
\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Parameter Learning: Maximum Likelihood}

\plitemsep 0.1in

\bci
\item Given a iid dataset $\set{X}= \{\vx_1, \ldots, \vx_n \},$ the log-likelihood is:
\aleq{
\cL(\vth) = \log p(\set{X} | \vth) = \sum_{n=1}^N \log p(\vx_n|\vth) = \sum_{n=1}^N \log \sum_{k=1}^K \pi_k
\cN(\vx_n | \vmu_k,\msig_k)
}

\item $\vth_{\ml} = \arg \min_{\vth} (-\cL(\vth))$
\item Necessary condition for $\vth_\ml$: $\dfrac{d\cL}{d\vth}\Big|_{\vth_\ml} = 0$

\item However, the closed-form solution of $\vth_\ml$ does not exist, so we rely on an iterative algorithm (also called EM algorithm).

\item We show the algorithm first, and then discuss how we get the algorithm.
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Responsibilities}

\plitemsep 0.1in

\bci
\item \defi \bluef{Responsibilities.} Given $n$-th data point $\vx_n$ and the parameters  $(\vmu_k, \msig_k, \pi_k: k=1, \ldots, K)$, 
$$
r_{nk}=  \frac{\pi_k \cN(\vx_n | \vmu_k, \msig_k)}{ \sum_{j}\pi_j \cN(\vx_n | \vmu_j, \msig_j) }
$$

\item How much is each component $k$ responsible, if the data $\vx_n$ is sampled from the current mixture model? 

\item $\vec{r}_n = (r_{nk}: k=1, \ldots, K)$ is a probability distribution, so $\sum_{k=1}^K r_{nk} =1$

\item Soft assignment of $\vx_n$ to the $K$ mixture components
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{EM Algorithm:  MLE in Gaussian Mixture Models}

\small
\myblock{EM for MLE in Gaussian Mixture Models}
{
\bce[\red \bf S1.]
\item Initialize $\vmu_k, \msig_k, \pi_k$

\item \bluef{\bf E-step:} Evaluate responsibilities $r_{nk}$ for every data point $\vx_n$ using the current  $\vmu_k, \msig_k, \pi_k$:
$$
\greenf{r_{nk}}=  \frac{\pi_k \cN(\vx_n | \vmu_k, \msig_k)}{ \sum_{j}\pi_j \cN(\vx_n | \vmu_j, \msig_j) }, \quad \greenf{N_k} = \sum_{n=1}^N \greenf{r_{nk}}
$$

\item \bluef{\bf M-step:} Reestimate parameters $\vmu_k, \msig_k, \pi_k$ using the current 
responsibilities $r_{nk}$:
\aleq{
\orangef{\vmu_k} = \frac{1}{\greenf{N_k}} \sum_{n=1}^N \greenf{r_{nk}} \vx_n, \ \orangef{\msig_k} = \frac{1}{\greenf{N_k}} 
\sum_{n=1}^N \greenf{r_{nk}} (\vx_n - \vmu_k)\trans{(\vx_n - \vmu_k)}, \ \orangef{\pi_k} = \frac{\greenf{N_k}}{N},
}
and go to \redf{\bf S2.}
\ece
}
\vspace{-0.3cm}
- The update equation in \bluef{\bf M-step} is still mysterious, which will be covered later.

% \vspace{-0.5cm}
% \plitemsep 0.1in
% \bci
% \item 
% \eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: EM Algorithm}

\mypic{0.7}{L11_em_ex.png}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{M-Step: Towards the Zero Gradient}

\plitemsep 0.07in

\bci
\item Given $\cX$ and $r_{nk}$ from E-step, the new updates of $\vmu_k$, $\msig_k$, $\pi_k$ should be made, such that the followings are satisfied:
\aleq{
\pd{\cL}{\vmu_k} &= \trans{\vec{0}} \Longleftrightarrow \sum_{n=1}^N \pd{\log p(\vx_n | \vth)}{\vmu_k} = \trans{\vec{0}} \cr
\pd{\cL}{\msig_k} &= \vec{0} \Longleftrightarrow \sum_{n=1}^N \pd{\log p(\vx_n | \vth)}{\msig_k} = \vec{0} \cr\pd{\cL}{\pi_k} &= 0 \Longleftrightarrow \sum_{n=1}^N \pd{\log p(\vx_n | \vth)}{\pi_k} = 0
}

\item Nice thing: the new updates of $\vmu_k$, $\msig_k$, $\pi_k$ are all expressed by the responsibilities $[r_{nk}]$ 

\item Let's take a look at them one by one!
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{M-Step: Update of $\vmu_k$}
\mycolorbox{ 
$$
\vmu_k^{\text{new}} = \frac{\sum_{n=1}^N r_{nk} \vx_n}{\sum_{n=1}^N r_{nk}}, k=1,\ldots, K
$$
}

\plitemsep 0.07in
\bci
\item 
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{M-Step: Update of $\msig_k$}
\mycolorbox{ 
$$
\msig_k^{\text{new}} = \frac{1}{N_k} 
\sum_{n=1}^N r_{nk} (\vx_n - \vmu_k)\trans{(\vx_n - \vmu_k)}, k=1,\ldots, K
$$
}

\plitemsep 0.07in
\bci
\item 
\eci
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{M-Step: Update of $\pi_k$}
\mycolorbox{ 
$$
\pi_k^{\text{new}} = \frac{\sum_{n=1}^N r_{nk}}{N}, k=1,\ldots, K
$$
}

\plitemsep 0.07in
\bci
\item 
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L11(3)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Gaussian Mixture Model}
\item \grayf{Parameter Learning: MLE}
\item \redf{Latent-Variable Perspective for Probabilistic Modeling}
\item \grayf{EM Algorithm}
\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Latent-Variable Perspective}

\plitemsep 0.07in
\bci
\item Justify some ad hoc decisions made earlier
\item Allow for a concrete interpretation of the responsibilities as \bluef{posterior distributions}
\item Iterative algorithm for updating the model parameters can be derived in a principled manner
\eci
\vspace{-0.9cm}
\mypic{0.3}{L11_gmm_gm.png}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Generative Process}

\plitemsep 0.07in
\bci
\item \redf{Latent variable $\vz$}: \bluef{One-hot encoding} random vector $\vz = \trans{[z_1, \ldots, z_K]}$ consisting of $K-1$ many 0s and exactly one 1. 

\item An indicator rv $z_k=1$ represents whether \bluef{$k$-th component is used to generate the data sample} $\vx$ or not. 

\item $p(\vx | z_k=1) = \cN(\vx| \vmu_k,\msig_k)$
\item Prior for $\vz$ with $\pi_k = p(z_k =1)$
$$
p(\vz) = \vpi = \trans{[\pi_1, \ldots, \pi_K]}, \quad \sum_{k=1}^K \pi_k = 1
$$

\item Sampling procedure
\bce
\item Sample which component to use $z^{(i)} \sim p(\vz)$
\item Sample data according to $i$-th Gaussian $\vx^{(i)} \sim p(\vx | z^{(i)})$
\ece
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Joint Distribution, Likelihood, and Posterior (1)}

\plitemsep 0.1in
\bci
\item Joint distribution
$$
p(\vx,\vz) = \colvec{p(\vx, z_1=1) \\ \vdots \\ p(\vx, z_K=1)} = 
\colvec{p(\vx| z_1=1)p(z_1=1) \\ \vdots \\ p(\vx|z_K=1)p(z_K=1)}=
\colvec{\pi_1\cN(\vx|\vmu_1,\msig_1) \\ \vdots \\ \pi_K\cN(\vx|\vmu_K,\msig_K)}
$$

\item Likelihood for an arbitrary single data $\vx$: By summing out all latent variables\footnote{In probabilistic PCA, $\vz$ was continuous, so we integrated them out.},
\aleq{
p(\vx | \vth) &= \sum_{\vz} p(\vx|\vth,\vz)p(\vz|\vth) = \sum_{k=1}^K p(\vx|\vth,z_k=1)p(z_k=1|\vth) 
= \sum_{k=1}^K \pi_k \cN(\vx|\vmu_k, \msig_k)
}
\item For all the data samples $\cX,$ the log-likelihood is:
\aleq{
 \log p(\set{X} | \vth) &= \sum_{n=1}^N \log p(\vx_n|\vth) = \sum_{n=1}^N \log \sum_{k=1}^K \pi_k
 \cN(\vx_n | \vmu_k,\msig_k) \hspace{2cm} \lecturemark{\text{Compare: Page 7}} 
}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Joint Distribution, Likelihood, and Posterior (2)}

\plitemsep 0.1in
\bci

\item Posterior for the $k$-th $z_k$, given an arbitrary single data $\vx$:
\aleq{
p(z_k=1 | \vx) = \frac{p(z_k=1)p(\vx|z_k=1)}{\sum_{j=1}^K p(z_j=1)p(\vx|z_j=1)}
= \frac{\pi_k
 \cN(\vx | \vmu_k,\msig_k)}{\sum_{j=1}^K\pi_j
 \cN(\vx | \vmu_j,\msig_j)}
}
\item Now, for all data samples $\set{X},$ each data $\vx_n$ has $\vz_n= \trans{[z_{n1}, \ldots, z_{nK}]},$ but with the same prior $\vpi.$
\aleq{
p(z_{nk}=1 | \vx_n) = \frac{p(z_{nk}=1)p(\vx_n|z_{nk}=1)}{\sum_{j=1}^K p(z_{nj}=1)p(\vx_n|z_{nj}=1)}
= \frac{\pi_k
 \cN(\vx_n | \vmu_k,\msig_k)}{\sum_{j=1}^K\pi_j
 \cN(\vx_n | \vmu_j,\msig_j)} = r_{nk}
}

\item Responsibilities are mathematically interpreted as \bluef{posterior distributions.}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L11(4)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Gaussian Mixture Model}
\item \grayf{Parameter Learning: MLE}
\item \grayf{Latent-Variable Perspective for Probabilistic Modeling}
\item \redf{EM Algorithm}
\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Revisiting EM Algorithm for MLE}

\mytwocols{0.5}
{
\small
\bce[\red \bf S1.]
\item Initialize $\vmu_k, \msig_k, \pi_k$

\item \bluef{\bf E-step:} 
$$
r_{nk}=  \frac{\pi_k \cN(\vx_n | \vmu_k, \msig_k)}{ \sum_{j}\pi_j \cN(\vx_n | \vmu_j, \msig_j) } 
$$

\item \bluef{\bf M-step:} Update $\vmu_k, \msig_k, \pi_k$ using $r_{nk}$
and go to \redf{\bf S2.}
\ece
}
{
\small
\bci
\item \bluef{\bf E-step.} \orangef{Expectation} over $\vz | \vx, \vth^{(t)}$: 
Given the current $\vth^{(t)} = (\vmu_k, \msig_k, \pi_k),$ calculates the expected log-likelihood
\aleq{
Q(\vth|\vth^{(t)}) &= \expecti{\vz|\vx,\vth^{(t)}}{\log p(\vx,\vz | \vth)} \cr
& = \int \log p(\vx,\vz | \vth) p(\vz|\vx,\vth^{(t)})\text{d}\vz
}

\item \bluef{\bf M-step.} \orangef{Maximization} of the computation results in E-step for the new model parameters.  

\eci
}

\bci
\item Only guarantee of just local-optimum because the original optimization is not necessarily a convex optimization. \hfill \lecturemark{L7(4)}
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Other Issues}

\plitemsep 0.1in
\bci
\item Model selection for finding a good $K$, e.g., using nested cross-validation

\item Application: Clustering
\bci
\item K-means: Treat the means in GMM as cluster centers and ignore the covariances. 
\item K-means: hard assignment, GMM: soft assignment
\eci

\item EM algorithm: Highly generic in the sense that it can be used for parameter learning in general latent-variable models

\item Standard criticism for MLE exists such as overfitting. Also, fully-Bayesian approach assuming some priors on the parameters is possible, but not covered in this notes. 

\item Other density estimation methods
\bci
\item Histogram-based method: non-parametric method
\item Kernel-density estimation: non-parametric method
\eci
\eci
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\vspace{2cm}
\LARGE Questions?


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Review Questions}
% \tableofcontents
%\plitemsep 0.1in
\bce[1)]
\item 

\ece
\end{frame}


\end{document}

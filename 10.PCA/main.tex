%\pdfminorversion=4
\documentclass[handout,fleqn,aspectratio=169]{beamer}

\input{../myhead}

\title[]{Lecture 10: Dimensionality Reduction with\\ Principal Component Analysis}
\author{Yi, Yung (이융)}
\institute{Mathematics for Machine Learning\\ \url{https://yung-web.github.io/home/courses/mathml.html}
\\KAIST EE}
\date{\today}


\input{../mymath}
\input{../mymacro}


%\addtobeamertemplate{footline}{\rule{0.94\paperwidth}{1pt}}{}

\begin{document}

\input{../mydefault}



% START START START START START START START START START START START START START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Warm-Up}

{\Large Please watch this tutorial video by Luis Serrano on PCA.}

\bigskip

\bigskip

\url{https://www.youtube.com/watch?v=g-Hb26agBFg}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item Problem Setting 
\item Maximum Variance Perspective 
\item Projection Perspective 
\item Eigenvector Computation and Low-Rank Approximations 
\item PCA in High Dimensions 
\item Key Steps of PCA in Practice 
\item Latent Variable Perspective

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L10(1)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \redf{Problem Setting }
\item \grayf{Maximum Variance Perspective 
\item Projection Perspective 
\item Eigenvector Computation and Low-Rank Approximations 
\item PCA in High Dimensions 
\item Key Steps of PCA in Practice 
\item Latent Variable Perspective}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Dimensionality Reduction}

\plitemsep 0.07in

\mypic{0.5}{L10_dr_ex.png}
\bci 
\item High-dimensional data
\bci
\item hard to analyze and visualize
\item Often, overcomplete and many dimensionas are redundant
\eci
\item Compact data representation is always preferred just like compression.
\item PCA (Principal Component Analysis) is a representative method.
\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Housing Data}

\plitemsep 0.1in

\bci 
\item 5 dimensions 
\bce
\redf{
\item Size
\item Number of rooms
\item Number of bathrooms
}
\greenf{
\item Schools around
\item Crime rate}
\ece

\item 2 dimensions
\bci
\item \redf{Size feature}
\item \greenf{Location feature}
\eci
\eci


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{PCA Algorithm}

\mycolorbox{
\bce[\bf \red S1.]
\item \bluef{Centering.} Centering the data by subtracting mean
\item \bluef{Standardization.} Divide the data points by the standard deviation for every dimension (original feature) $d=1, \ldots, D$
\item \bluef{Eigenvalue/vector.} Compute the $M$-largest eigenvalues and the eigenvectors of the data covariance matrix ($M$ is the dimension that needs to be reduced)
\item \bluef{Projection.} Project all data points onto the space defined by the eigenvectors (i.e., principal subspace). 

\item Undo standardization and centering.
\ece
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{PCA Illustration}


\mypic{0.7}{L10_pca_algorithm.png}
% \vspace{-0.5cm}
% \raggedleft
% {\scriptsize source: Youtube channel by Luis Serrano}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{PCA in one picture}


% \mypic{0.85}{L9_PCA_onepicture.png}
% \vspace{-0.5cm}
% \raggedleft
% {\scriptsize source: Youtube channel by Luis Serrano}

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Data Matrix and Data Covariance Matrix}

\plitemsep 0.05in

\bci 
\item $N$: number of samples, $D$: number of measurements (or original features)
\item iid dataset $\cX = \{\vx_1, \ldots, \vx_N \}$ whose mean is $\vec{0}$ (well-centered), where each $\vx_i \in \realD,$ and its corresponding data matrix 
$$\mX = \rowvec{\vx_1 & \cdots & \vx_N} =\generalmatrix{x}{N}{D}
\in \real^{D \times N}$$

\item \bluef{(data) covariance matrix}  
\mycolorbox{
$$\mS = \dfrac{1}{N} \mX\trans{\mX} = \dfrac{1}{N} \sum_{n=1}^N \vx_n \trans{\vx}_n \in \real^{D\times D}$$
}

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Covariance Matrix and Data Covariance Matrix}

\plitemsep 0.1in

\bci
\item Covaiance matrix for a random vector $\vY = \trans{(Y_1, \ldots, Y_D)},$ \hfill \lecturemark{L6(4)}
$$
\msig_{\vY}  = \begin{nmat}
\cov{Y_1,Y_1} & \cov{Y_1,Y_2} & \cdots \cov{Y_1,Y_D} \cr
\vdots & \vdots & \vdots \cr
\cov{Y_D,Y_1} & \cov{Y_n,Y_2} & \cdots \cov{Y_D,Y_D} \cr
\end{nmat}
$$

\item Data convariance matrix $\mS \in \real^{D\times D}$

\bci
\item Each $Y_i$ has $N$ samples $\rowvec{x_{i,1} & \cdots & x_{i,N}}$
\eci
\begin{eqnarray*}
\mS_{ij}= \cov{Y_i, Y_j} &=& \frac{1}{N}\sum_{k=1}^N x_{i,k} \cdot x_{j,k} \cr
&=&\text{ average covariance (over samples) btwn feastures $i$ and $j$}
\end{eqnarray*}

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Code: Low Dimensional Representation}

\plitemsep 0.07in

\bci 
\item Low-dimensional compressed representation, also called \bluef{code}:
$$
\vz_n = \trans{\mB}\vx_n \in \real^{M},
$$
where the projection\footnote{In \lecturemark{L3(8)}, the coordinate in the projected space becomes $\vlam = \inv{(\trans{\mB}\mB)}\trans{\mB} \vec{x},$ which is simply $\trans{\mB} \vec{x}$ for orthonormal bases $\mB.$} matrix is $\mB \eqdef (\vb_1, \ldots, \vb_M) \in \realDM,$

\item Assume that the columns of $\mB$ are orthonormal, i.e., $\trans{\vb}_i \vb_j = 0$ if $i \neq j,$ and 
$\trans{\vb}_i \vb_i = 1$ if $i = j.$

\item Seek an $M$-dimensional subspace $U \subset \realD,$ $\dim(U) = M < D$ onto which we project data

\item $\tilde{\vx}_n \in \realD$: projected data, $\vz_n$: their coordinates w.r.t. the basis vectors of $\mB.$

% \item Goal: Find low-dimensional representations that retain as much information as possible and minimize the compression loss.
\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{PCA: Encoder and Decoder Viewpoint}

\mypic{0.35}{L10_pca_picture.png}
\vspace{-0.7cm}
\plitemsep 0.07in
\bci 
\item Find a suitable matrix $B$ such that $\vz = \trans{\mB}\vx$ and $\tilde{\vx} = \mB \vz$
\item $\trans{\mB}$: encoder, $\mB$: decoder

\item \exam MNIST dataset
\bci
\item handwritten digits, $N=60,000$ data samples, $D=28\times28 = 784$ pixels
\eci
\mypic{0.5}{L10_mnist.png}

\eci


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L10(2)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Problem Setting }
\item \redf{Maximum Variance Perspective} 
\item \grayf{Projection Perspective 
\item Eigenvector Computation and Low-Rank Approximations 
\item PCA in High Dimensions 
\item Key Steps of PCA in Practice 
\item Latent Variable Perspective}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Idea}

\plitemsep 0.1in

\bci 

\item Information content in the data

\bci
\item space filling
\item information in the data by looking at how much data is spread out
\eci

\item PCA
\bci
\item a dimensinoality reduction algorithm that maximizes the variance in the low-dimensional data representation.
\eci
\eci
\vspace{-0.4cm}
\mypic{0.4}{L10_variance_diff.png}
\vspace{-0.5cm}
\raggedleft
{\scriptsize source: Youtube channel by Luis Serrano}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Matrix Again: $B,$ $\vz_n,$ and $\vx_n$}

\plitemsep 0.1in

\small
\bci 

\item $\mB = \rowvec{\vb_1 & \vb_2 & \ldots & \vb_M},$ where $\vb_i \in \realD$ and $\mB \in \realDM$

\item $\trans{\mB} = \colvec{\trans{\vb}_1 \\ \vdots \\ \trans{\vb}_M} \in \realMD,$ $\trans{\vb}_i \in \real^{1 \times D},$ $\vx_i \in \real^{D \times 1}$


\item $\vz_n = \colvec{z_{1n} \\ \vdots \\ z_{Mn}} = \trans{\mB}\vx_n = 
\colvec{\trans{\vb}_1 \\ \vdots \\ \trans{\vb}_M}  \vx_n = \colvec{\trans{\vb}_1  \vx_n\\ \vdots \\ \trans{\vb}_M  \vx_n}$ 

\item $z_{in}$: new coordinate (for $\vx_n$) in the projected space by the basis $\vb_i$
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What We Will Do Is ...}

\plitemsep 0.05in

\bci 

\item \bluef{Goal}: Find the orthonormal bases $\mB = \rowvec{\vb_1 & \vb_2 & \ldots & \vb_M}$ that maximizes the variance.

\item \bluef{Result}: For the $M$-largest eigenvalues $\lambda_1, \ldots, \lambda_M$ of the data covariance matrix $\mS$, their corresponding $M$ eigenvectors become $\vb_1, \ldots, \vb_M$  

\item \question Why data covariance matrix? Why eigenvectors ordered by their eigenvalues?

\item Strategy: Induction
\bci
\item[Step 1.] We seek a single vector $\vb_1$ that maximizes the variance of the projected data, assuming that we project the data onto an 1D line. We show that $\vb_1$ is the \bluef{eigenvector of the largest eigenvalue.} 
\item[Step k.] Suppose that we found $\vb_1, \ldots, \vb_{k-1}$ for the variance maximization. Then, we seek $\vb_k$ that maximizes the variance of the projected data onto $k$-D plain with the constraint that $\vb_k$ is orthogonal to $\vb_1, \ldots, \vb_{k-1}.$ We prove that $\vb_k$ is the \bluef{eigenvector of the $k$-th largest eigenvalue.} 
\eci

\eci


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step 1: Finding $\vb_1$ (1)}

\plitemsep 0.1in

\small
\bci 

\item Variance (over $N$ sample data) of the first coordinate $z_1$ of $\vz \in \realM,$ so that
$$
V_1 \eqdef \var{z_1} = \frac{1}{N} \sum_{n=1}^N z_{1n}^2, \quad z_{1n} = \trans{\vb}_1  \vx_n
$$
where $z_{1n}$ ($z_{in}$) is the first ($i$-th) coordinate of the low-dimensional representation $\vz_n$ of $\vx_n$
\aleq{
V_1 = \frac{1}{N} \sum_{n=1}^N (\trans{\vb}_1  \vx_n)^2 = \frac{1}{N} \sum_{n=1}^N \trans{\vb}_1\vx_n 
\trans{\vx}_n \vb_1 = \trans{\vb}_1 \Big(\frac{1}{N} \sum_{n=1}^N \vx_n 
\trans{\vx}_n\Big) \vb_1 = \trans{\vb}_1 \mS \vb_1
}

\item \bluef{Find $\vb_1$ that maximizes $V_1.$}

\mycolorbox{
$\max_{\vb_1} \trans{\vb}_1 \mS \vb_1, \quad \text{subject to} \quad \norm{\vb_1}^2 = 1$ 
}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step 1: Finding $\vb_1$ (2)}


\plitemsep 0.1in

\small
\bci 

\item Optimization problem
\mycolorbox{
$\max_{\vb_1} \trans{\vb}_1 \mS \vb_1, \quad \text{subject to} \quad \norm{\vb_1}^2 = 1$ 
}

\item Using the Lagrange multiplier method, we get: \hfill \lecturemark{L7(2), L7(4)}
\mycolorbox{
$\mS \vb_1 = \lambda_1 \vb_1, \quad \trans{\vb}_1\vb_1 = 1$ $\implies$ $\lambda_1$: eigenvalue, $\vb_1$: eigenvector of $\mS$ 
}
\item Then, $V_1 = \trans{\vb}_1 \mS \vb_1 = \lambda_1 \trans{\vb}_1 \vb_1 = \lambda_1$ (the variance $V_1$ is the eigenvalue of $S$)

\item To maximize the variance, we take the largest eigenvalue, and the corresponding eigenvector is called the \bluef{(first) principal component}.

% \item \bluef{Decoding}: $\tilde{\vx}_n = \vb_1 z_{1n} = \vb_1 \trans{\vb}_1 \vx_n$ 
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step $k$: Finding $\vb_k$ (1)}

\plitemsep 0.1in

\bci 
\item Finding $k$-th principal component: Solving the following optimization problem
\mycolorbox{
$\max_{\vb} \trans{\vb} \mS \vb, \quad \text{subject to} \quad 
\trans{\vb}\vb = 1 \ \text{and} \ \trans{\vb}\vb_i, \ i=1, \ldots, k-1$ 
}
\item \redf{Claim.} The solution of the above is the eigenvector of $\mS$ corresponding to its $k$-th largest eigenvalue. 

\item \redf{Proof.} 
\small
By induction hypothesis, $\vb_1, \ldots, \vb_k$ are the orthonormal eigenvectors of $\mS.$ Denote the $i$-th largest eigenvalue of $\mS$ by $\lambda_i,$ where note that $\mS \vb_i = \lambda_i \vb_i.$

The lagrangian of the objective function is:
\aleq{
\cL(\vb) = \trans{\vb}\mS \vb - \lambda (\trans{\vb}\vb -1) + \sum_{i=1}^k \eta_i \trans{\vb}\vb_i 
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step $k$: Finding $\vb_k$ (2)}

\small
\plitemsep 0.07in
\bci 
\item Letting the solution be denoted by $\vb_{k+1}$, the first-order necessary condition for optimality is: 
\vspace{-0.3cm}
\aleq{
\grad \cL(\vb_{k+1}) &= 2\mS\vb_{k+1} - 2 \lambda \vb_{k+1} + \sum_{i=1}^k \eta_i \vb_i = \vec{0} \hspace{5cm} (*)
}
\item Now, for any $j \in \{1, \ldots, k \},$
\vspace{-0.3cm}
\aleq{
0 &= \trans{\vb}_j \grad \cL(\vb_{k+1}) = 2\trans{\vb}_j\mS\vb_{k+1} - 2 \lambda \trans{\vb}_j\vb_{k+1} + \sum_{i=1}^k \eta_i \trans{\vb}_j\vb_i
= 2\trans{(\mS \vb_j)}\vb_{k+1} + \eta_j \cr
&=  2\trans{(\lambda \vb_j)}\vb_{k+1} + \eta_j = 2\lambda \trans{\vb_j}\vb_{k+1} + \eta_j = \eta_j
}
\item From $\eta_j =0$ and (*), $\mS \vb_{k+1} = \lambda \vb_{k+1}.$ $\implies$ $\lambda$ is an eigenvalue and its corresponding eigenvector is $\vb_{k+1}.$ 

\item Note that the objective function is $\lambda,$ because $\trans{\vb}\mS \vb = \lambda \trans{\vb} \vb$. 
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step $k$: Finding $\vb_k$ (3)}

\small
\plitemsep 0.1in
\bci 

\item \question How can we choose the largest $\lambda$ with the constraint that $\vb_{k+1} \perp (\vb_1, \ldots \vb_k)$? 


\item Clearly, if $\vb_{k+1}$ is equal to any of these eigenvectors (up to sign), the constraint will be violated, so, to maximize $\lambda,$ $\vb_{k+1}$ should be a unit eigenvector of $\mS$ corresponding to $(k+1)$-th largest eigenvalue. 

\item By spectral theorem, we can choose this vector in such a way that it is orthogonal to $\vb_1, \ldots, \vb_k.$ \hfill \lecturemark{L4(4)}
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L10(3)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Problem Setting
\item Maximum Variance Perspective} 
\item \redf{Projection Perspective} 
\item \grayf{Eigenvector Computation and Low-Rank Approximations 
\item PCA in High Dimensions 
\item Key Steps of PCA in Practice 
\item Latent Variable Perspective}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Storyline}

\plitemsep 0.07in

\bci

\item An ordered orthonormal bais (ONB) $B=(\vb_1, \ldots, \vb_D)$
\item $\mB = \rowvec{\vb_1 & \vb_2 & \ldots & \vb_M},$ where $\vb_i \in \realD$ and $\mB \in \realDM$
\item Encoding: $\vz_n = \phi(\vx_n)$ for some mapping $\phi(\cdot)$ 
\item Decoding: $\tilde{\vx}_n \eqdef \mB \vz_n = \sum_{m=1}^M z_{mn}\vb_m$
\item Goal: find the best linear projection of $\set{X}=\{\vx_1, \ldots, \vx_N \}$ onto a lower-dimensional subspace $U$ (also, called \bluef{principal subspace}) of $\realD$ with $\dimm{U}=M$.
\item Formally, minimize the following \bluef{reconstruction error}
\mycolorbox{
$$
J_M \eqdef \frac{1}{N} \sum_{n=1}^N \norm{\vx_n - \tilde{\vx}_n}^2,
$$
where the variables are $(\vz_n: n=1, \ldots, N)$ and $(\vb_1, \ldots, \vb_M)$}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Two-step Approach}

\plitemsep 0.2in

\bce[\bf Step 1.]

\item We optimize the coordinate $\vz_n$ in the space $U$ for a given ONB $(\vb_1, \ldots, \vb_M)$

\item Then, we find the optimal ONB, knowing the optimal $\vz_n$ in \bluef{\bf Step 1.}
\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step 1: Optimal coordinate $\vz_n$ for a given ONB}

\plitemsep 0.03in

\bci

\item Intuition: \bluef{Orthogonal} projection \hfill \lecturemark{L3(8)} 
\mycolorbox{
\vspace{-0.2cm}
\aleq{
\bluef{Result}: \tilde{\vx}_n = \mB\inv{(\trans{\mB}\mB)}\trans{\mB} \vx_n = \mB\trans{\mB} \vx_n = \mB \vz_n, \redf{\vz_n = \trans{\mB} \vx_n}
}
}
\item \redf{Proof.}
Assume an ONB $(\vb_1, \ldots, \vb_M)$. Noting that $J_M$ is a function of $\tilde{\vx}_n$ and $\tilde{\vx}_n$ is a function of $\vz_n,$
\vspace{-0.3cm}
\small
\aleq{
\pd{J_M}{z_{in}} &= \pd{J_M}{\tilde{\vx}_n} \pd{\tilde{\vx}_n}{z_{in}}, \quad 
\pd{J_M}{\tilde{\vx}_n} = - \frac{2}{N}\trans{(\vx_n - \tilde{\vx}_n)}, \quad 
\pd{\tilde{\vx}_n}{z_{in}} = \pd{}{z_{in}}\left( \sum_{m=1}^M z_{mn}\vb_m \right )= \vb_i  \cr
\pd{J_M}{z_{in}} &=- \frac{2}{N}\trans{(\vx_n - \tilde{\vx}_n)} \vb_i = 
- \frac{2}{N}\trans{\Bigg(\vx_n -  \sum_{m=1}^M z_{mn}\vb_m \Bigg)} \vb_i 
\stackrel{\text{ONB}}{=} - \frac{2}{N}(\trans{\vx}_n\vb_i -z_{in} \trans{\vb}_i \vb_i) \cr
&= - \frac{2}{N}(\trans{\vx}_n\vb_i -z_{in} )
}
\item $z_{in} = \trans{\vx}_n \vb_i = \trans{\vb}_i \vx_n$ for $i=1, \ldots, M$ and $n=1, \ldots, N$  \hfill (ortho. proj. onto 1D \lecturemark{L3(8)})
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step 2: Finding Optimal Basis $(\vb_1, \ldots, \vb_M)$ (1)}

\plitemsep 0.1in

\bci

\item The difference: $\displaystyle \vx_n - \tilde{\vx}_n = \left(\sum_{j=M+1}^D \vb_j \trans{\vb}_j \right) \vx_n = \sum_{j=M+1}^D (\trans{\vx}_n \vb_j)\vb_j$

\aleq{
\tilde{\vx}_n &= \sum_{m=1}^M z_{mn}\vb_m \stackrel{\blue \text{Step 1}}{=} 
\sum_{m=1}^M (\trans{\vx}_n \vb_m  )\vb_m = \sum_{m=1}^M \vb_m (\trans{\vb}_m \vx_n ) =
\left(\sum_{m=1}^M \vb_m \trans{\vb}_m \right) \vx_n \cr
\vx_n &= \sum_{d=1}^D z_{dn}\vb_d = \left(\sum_{m=1}^M \vb_m \trans{\vb}_m \right) \vx_n + 
\left(\sum_{j=M+1}^D \vb_j \trans{\vb}_j \right) \vx_n
}

\item The projection of the data point onto the \bluef{orthogonal complement} of the principal subspace \hfill \lecturemark{L3(6)}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Step 2: Finding Optimal Basis $(\vb_1, \ldots, \vb_M)$ (2)}

\plitemsep 0.1in

\bci

\item[] 
\aleq{
J_M &= \frac{1}{N} \sum_{n=1}^N \norm{\vx_n - \tilde{\vx}_n}^2 = \frac{1}{N} \sum_{n=1}^N 
\norm{\sum_{j=M+1}^D (\trans{\vb}_j \vx_n)\vb_j}^2 = \frac{1}{N} \sum_{n=1}^N \sum_{j=M+1}^D (\trans{\vb}_j \vx_n)^2 \cr
&= \frac{1}{N} \sum_{n=1}^N \sum_{j=M+1}^D \trans{\vb}_j \vx_n \trans{\vx}_n \vb_j 
= \sum_{j=M+1}^D \trans{\vb}_j\left (\frac{1}{N} \sum_{n=1}^N \vx_n \trans{\vx}_n   \right ) \vb_j = 
\sum_{j=M+1}^D \trans{\vb}_j\mS \vb_j \cr
}
- \bluef{minimizing} the squared reconstruction error \redf{=} \bluef{minimizing} the variance when projected onto the orthogonal complement of the principal subspace \redf{=} \bluef{maximizing} the variance of the projection in the principal subspace

\item $J_M = \sum_{j=M+1}^D \lambda_j$ (because of the projection). To minimize this error, we need to choose the smallest $D-M$ eigenvalues, which means that we need to choose the $M$ largest eigenvalues and take their corresponding eigenvectors for projection.   

\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L10(4)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Problem Setting
\item Maximum Variance Perspective 
\item Projection Perspective} 
\item \redf{Eigenvector Computation and Low-Rank Approximations} 
\item \grayf{PCA in High Dimensions 
\item Key Steps of PCA in Practice 
\item Latent Variable Perspective}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Eigenvector Computation}

\plitemsep 0.1in

\bci

% \item data covariance matrix  
% \mycolorbox{
% $$\mS = \dfrac{1}{N} \mX\trans{\mX} = \dfrac{1}{N} \sum_{n=1}^N \vx_n \trans{\vx}_n \in \real^{D\times D}$$
% }

% \item Two approaches: EVD and SVD

\item Approach 1: \bluef{EVD} \hfill \lecturemark{L4(4)}
\bci
\item Perform an eigendecomposition and compute the eigenvalues and eigenvectors of the symmetric matrix $\mS$ directly. 
\eci

\item Approach 2: \bluef{SVD} \hfill \lecturemark{L4(5)}
\bci
\item SVD of the data matrix $\mX$: $\mX = \mU \msig \trans{\mV}$ $([D\times N] = [D \times D]\cdot [D \times N] \cdot [N \times N])$
\item $\mU$ and $\trans{\mV}$: orthogonal matrices, $\msig$: only nonzero entries are the singular values $\sigma_{ii} \geq 0.$
\aleq{
\mS = \dfrac{1}{N} \mX\trans{\mX} = \frac{1}{N} \mU \msig \trans{\mV} \mV \trans{\msig} \trans{\mU} \stackrel{(\trans{\mV} = \inv{\mV})}{=} \frac{1}{N} \mU \msig \trans{\msig} \trans{\mU} 
}
\item The columns of $\mU$ are the eigenvectors of $\mX\trans{\mX}$ (thus $\mS$)

\item The eigenvalues $\lambda_d$ of $\mS$ are related to the singular values of $\mX$: 
$\lambda_d = \dfrac{\sigma_d^2}{N}$
\eci
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{PCA as Low-Rank Matrix Approximations}

\plitemsep 0.1in

\bci

\item In SVD, $\mU$ corresponds to the projection matrix $\mB$, so that we  maximize the variance of the projected data or minimize the average squared reconstruction error. 


\item Consider the best rank-$M$ approximation
$$
\tilde{\mX}_M \eqdef \arg \min_{\rk{\mA} = M} \norm{\mX - \mA}_2
$$

\item From Eckart-Young Theorem, by truncating the SVD at the top-$M$ singular value, we obtain the reconstructed data matrix $\tilde{\mX}_M$ as: \hfill \lecturemark{L4(5), L4(6)}
$$
\tilde{\mX}_M = \overbrace{\mU_M}^{D \times M} \overbrace{\msig_M }^{M \times M} \overbrace{\trans{\mV_M}}^{M \times N} \ \Longleftrightarrow \ \tilde{\mX}_M = \sum_{i=1}^M \sigma_i \vec{u}_i \trans{\vec{v}_i},
$$
where $\sigma_i$ is the $i$-th singular value. 
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L10(4)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Problem Setting
\item Maximum Variance Perspective 
\item Projection Perspective} 
\item Eigenvector Computation and Low-Rank Approximations 
\item \redf{PCA in High Dimensions 
\item Key Steps of PCA in Practice} 
\item \grayf{Latent Variable Perspective}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{PCA as Low-Rank Matrix Approximations}

\plitemsep 0.05in

\bci

\item In some practical cases, $\mS = \frac{1}{N} \mX \trans{\mX} \in \real^{D \times D},$ where $D$ is pretty high.
\bci
\item \exam 100 $\times$ 100 pixel image: $D=10,000.$
\eci
\item What if $N << D$?
\bci
\item With no duplicate data, $\rk{\mS} = N,$ and $D-N+1$ eigenvalues are 0! $\implies$ no need to maintain $D \times D$ data covariance matrix. 

\eci

\item In PCA, $\mS \vb_m = \lambda_m \vb_m,$ $m=1, \ldots, M.$
\aleq{
\mS \vb_m = \frac{1}{N} \mX \trans{\mX}\vb_m = \lambda_m \vb_m \ \implies \ 
\frac{1}{N} \underbrace{\trans{\mX} \mX}_{N \times N} \underbrace{\trans{\mX}\vb_m}_{\eqdef \vc_m} = \lambda_m  \trans{\mX}\vb_m \Longleftrightarrow \frac{1}{N}\trans{\mX} \mX \vc_m = \lambda_m \vc_m
}
\item $\lambda_m$ is an eigenvalue of $\frac{1}{N}\trans{\mX} \mX$ with its associated eigenvector $\vc_m = \trans{\mX}\vb_m$

\item $\frac{1}{N}\trans{\mX} \mX \in \real^{N \times N}$, so much easier to compute the eigenstuff 
\item To recover the eigenvector of $\mS,$ by left-multiplying $X,$ we get $\frac{1}{N} \mX \trans{\mX} \mX \vc_m = \lambda_m \mX \vc_m$
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{PCA Algorithm}

\mycolorbox{
\bce[\bf \red S1.]
\item \bluef{Centering.} Centering the data by subtracting mean
\item \bluef{Standardization.} Divide the data points by the standard deviation for every dimension (original feature) $d=1, \ldots, D$
\item \bluef{Eigenvalue/vector.} Compute the $M$-largest eigenvalues and the eigenvectors of the data covariance matrix ($M$ is the dimension that needs to be reduced)
\item \bluef{Projection.} Project all data points onto the space defined by the eigenvectors (i.e., principal subspace). 

\item Undo standardization and centering.
\ece
}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L10(7)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item \grayf{Problem Setting
\item Maximum Variance Perspective 
\item Projection Perspective 
\item Eigenvector Computation and Low-Rank Approximations 
\item PCA in High Dimensions 
\item Key Steps of PCA in Practice} 
\item \redf{Latent Variable Perspective}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Generative Modeling with Latent Variables}

\vspace{2cm}
\LARGE Please go back to \lecturemark{L8(4)} for the background on generative models via latent variable models (LVMs).

% \plitemsep 0.1in

% \myvartwocols{0.7}{0.29}{0.65}
% {
% \bci 
% \item Generative process
% \bci
% \item $\vz \sim p(\vz)$
% \item $\vx \sim p(\vx | \vz)$
% \eci
% \eci

% \bigskip
% \aleq
% {
% p(\vx) &= \int p(\vx,\vz)\text{d}\vz \cr
% &=\int p(\vx|\vz)p(\vz)\text{d}\vz
% }

% }
% {
% \vspace{-0.5cm}
% \mypic{0.4}{L10_latent.png}
% \vspace{-0.6cm}
% \raggedleft
% {\tiny Source: \url{https://dlvu.github.io/slides/dlvu.lecture06.pdf}}
% }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Probabilistic PCA: Linear Latent Models}

\plitemsep 0.07in

\bci 
\item $p(\vz) = \set{N}(\vec{0},\mI)$

\item A linear relationship between $\vz$ and $\vx$: For Guassian observation noise $\vep \sim \set{N}(\vec{0},\sigma^2 \mI)$ and affine mapping defined by $\mB \in \realDM$ and $\vmu \in \realD,$
$$
\vx = \mB\vz + \vmu + \vep \in \realD
$$

\item Conditional distribution for the links between latent and observed variables
\mycolorbox{
$p(\vx|\vz, \mB, \vmu, \sigma^2) = \set{N}(\vx|\mB\vz + \vmu, \sigma^2\mI)$
}
\item Data point generation: \bluef{ancestral sampling}
\bci
\item First, sample $\vz_n$ from $p(\vz)$
\item Then, use $\vz_n$ to generate a sample $\vx_n \sim p(\vx|\vz_n, \mB, \vmu, \sigma^2)$
\eci
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Probabilistic Model and Likelihood}

\plitemsep 0.07in

\bci 
\item Probabilistic model: joint distribution
\mycolorbox{
$p(\vx,\vz | \mB, \vmu, \sigma^2) = p(\vx|\vz, \mB, \vmu, \sigma^2)p(\vz)$
}

\item Likelihood
\mycolorbox{
\vspace{-0.2cm}
\aleq{
p(\vx | \mB, \vmu, \sigma^2) &= \int p(\vx|\vz, \mB, \vmu, \sigma^2)p(\vz)\text{d}\vz 
= \int \set{N}(\vx|\mB\vz + \vmu, \sigma^2\mI) \set{N}(\vz | 0, \mI)\text{d}\vz\cr
&= \set{N}(\vmu, \mB \trans{\mB} + \sigma^2 \mI)
}
}
\bci
\item Using the property of marginal and conditional Gaussians \hfill \lecturemark{L6(5)}
\eci
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Posterior Distribution}

\plitemsep 0.07in

\bci 
\item The joint Gaussian distribution $p(\vx,\vz | \mB, \vmu, \sigma^2)$ leads us to the posterior distribution

\mycolorbox{
\vspace{-0.2cm}
\aleq{
p(\vz |\vx) &= \set{N}(\vz | \vm, \mC), \ \text{where} \cr
\vm&= \trans{\mB}\inv{(\mB \trans{\mB} + \sigma^2 \mI)}(\vx -\vmu), \ \mC=\mI - \trans{\mB}\inv{(\mB \trans{\mB} + \sigma^2 \mI)} \mB
}
}
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Learning Probabilistic PCA: MLE}

\plitemsep 0.07in

\bci 
\item For data samples $\mX = (\vx_1, \ldots, \vx_N),$ we are able to compute the likelihood as: 
$$
\log p(\mX | \mB, \vmu, \sigma^2 ) = \sum_{n=1}^N \log p(\vx_n | \mB, \vmu, \sigma^2)
$$

% \item Then, we get the following MLE of the parmaeters
\mycolorbox{
\vspace{-0.2cm}
\aleq{
\bluef{\vmu_\ml} = \frac{1}{N}\sum_{n=1}^N \vx_n, \ \bluef{\mB_\ml} = \mU(\mat{\Lambda} -\sigma^2 \mI)^{1/2}\mat{R}, \ \bluef{\sigma_\ml} = \frac{1}{D-M} \sum_{j=M+1}^D \lambda_j, \ \text{where}
}
\bci
\item $\mU$ is a $D \times M$ matrix whose columns are eigenvectors of $\mS$
\item $\mat{\Lambda}$ is a $M \times M$ diagonal matrix whose elements are eigenvalues of $\mS$
\item $\mat{R}$ is an arbitrary orthogonal matrix (i.e., rotation)
\eci
}
\item In the noise-free limit where $\sigma \rightarrow 0,$ PPCA and PCA provide the identical solution.
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{PCA as Linear Auto-Encoder}


\mypic{0.35}{L10_pca_picture.png}
\vspace{-0.7cm}
\plitemsep 0.07in
\bci 
\item \bluef{Non-linear auto-encoder}: we replace the linear mapping of PCA with a non-linear mapping. An example is a deep auto-encoder with deep neutral networks.
\item \bluef{(Fully) Bayesian PCA}: place a prior on the model parameters and integrate them out, rather than having a point estimate.
\item \bluef{Factor analysis}: allow each observation dimension $d$ to have a different variance $\sigma_d^2$
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\vspace{2cm}
\LARGE Questions?


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Review Questions}
% \tableofcontents
%\plitemsep 0.1in
\bce[1)]
\item 

\ece
\end{frame}


\end{document}

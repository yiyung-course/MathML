%\pdfminorversion=4
\documentclass[handout,fleqn,aspectratio=169]{beamer}

\input{../myhead}

\title[]{Lecture 9: Linear Regression}
\author{Yi, Yung (이융)}
\institute{Mathematics for Machine Learning\\ \url{https://yung-web.github.io/home/courses/mathml.html}
\\KAIST EE}
\date{\today}


\input{../mymath}
\input{../mymacro}


%\addtobeamertemplate{footline}{\rule{0.94\paperwidth}{1pt}}{}

\begin{document}

\input{../mydefault}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Warm-Up}

{\Large Please watch this tutorial video by Luis Serrano on PCA.}

\bigskip

\bigskip

\url{https://www.youtube.com/watch?v=wYPUhge9w5c}

\end{frame}

% START START START START START START START START START START START START START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item  Problem Formulation 
\item  Parameter Estimation: ML 
\item  Parameter Estimation: MAP 
\item  Bayesian Linear Regression 
\item  Maximum Likelihood as Orthogonal Projection 

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(1)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item  \redf{Problem Formulation}
\item  \grayf{Parameter Estimation: ML 
\item  Parameter Estimation: MAP 
\item  Bayesian Linear Regression 
\item  Maximum Likelihood as Orthogonal Projection }

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Regression Problem}

\mypic{0.75}{L9_regression_ex.png}

\plitemsep 0.1in

\bci 

\item For some input values $x_n,$ we observe noisy function values $y_n = f(x_n) + \epsilon$

\item Goal: infer the function $f$ that generalizes well to function values at new inputs 

\item Applications: time-series analysis, control and robotics, image recognition, etc.
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Formulation}

\plitemsep 0.07in

\bci 


\item[] 
{\small
Notation for simplification (this is how the textbook uses)
\aleq{
\redf{p(y|\vx)} = p_{Y|\vX}(y | \vx), \quad Y \sim \set{N}(\mu,\sigma^2) \xrightarrow{\text{simplifies}} \set{N}(y \mid f(\vx), \sigma^2)
}
}
\item Assume: \bluef{linear} regression, \bluef{Gaussian} noise

\item $y = f(\vx) + \epsilon,$ where $\epsilon \sim \set{N}(0,\sigma^2)$


\item Likelihood: for $\vx \in \real^D$ and $y \in \real,$ $p(y \mid \vx) = \set{N}(y \mid f(\vx), \sigma^2)$ 


\item Linear regression with the parameter $\vth \in \realD,$ i.e., $f(\vx) = \trans{\vx}\vth$
$$
p(y \mid \vx) = \set{N}(y \mid \trans{\vx}\vth, \sigma^2) \Longleftrightarrow y = \trans{\vx}\vth + \epsilon, \quad \epsilon \sim \set{N}(0,\sigma^2)
$$

\mycolorbox
{
\centering
Prior with Gaussian nose: $p(y \mid \vx) = \set{N}(y \mid \trans{\vx}\vth, \sigma^2)$
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Parameter Estimation}

\plitemsep 0.2in

\bci 

\item Training set $\set{D} = \{(\vx_1, y_1), \ldots, (\vx_N,y_N) \}$\hspace{3cm} 
\myinlinepic{2.5cm}{L9_LR_gmodel.png}

\item Assuming iid $N$ data samples, the likelihood is factorized into:
$$
p(\set{Y} \mid \set{X},\vth) = \prod_{n=1}^N p(y_n \mid \vx_n, \vth) = \prod_{n=1}^N 
\set{N}(y_n \mid \trans{\vx}_n, \sigma^2),
$$
where $\set{X} = \{\vx_1,\ldots,\vx_n \}$ and $\set{Y} = \{y_1,\ldots,y_n \}$
\item Estimation methods: ML and MAP
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(2)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item  \grayf{Problem Formulation}
\item  \redf{Parameter Estimation: ML} 
\item  \grayf{Parameter Estimation: MAP 
\item  Bayesian Linear Regression 
\item  Maximum Likelihood as Orthogonal Projection }

\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MLE (Maximum Likelihood Estimation) (1)}

\plitemsep 0.1in

\bci 

\item $\vth_\ml = \arg \max_{\vth} p(\cY \mid \cX, \vth) = \arg \min_{\vth} \Big( -\log p(\cY \mid \cX, \vth) \Big)$ 
\item For Gaussian noise with $\mX = \trans{[\vx_1, \ldots, \vx_n]}$ and $\vy = \trans{[y_1, \ldots, y_n]},$
\aleq{
-\log p(\cY \mid \cX, \vth) &=  -\log \prod_{n=1}^N p(y_n \mid \vx_n, \vth) =  -\sum_{n=1}^N \log p(y_n \mid \vx_n, \vth) \cr 
& =  \frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \trans{\vx}_n \vth)^2 + \ \text{const} =  \frac{1}{2\sigma^2} \norm{\vy - \mX \vth}^2 + \ \text{const}
}

\mycolorbox
{
Negative-log likelihood for $f(\vx) = \trans{\vx}\vth + \set{N}(0,\sigma^2)$:
\vspace{-0.1cm}
$$
-\log p(\cY \mid \cX, \vth) = \frac{1}{2\sigma^2} \norm{\vy - \mX \vth}^2 + \ \text{const}
$$
}
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MLE (Maximum Likelihood Estimation) (2)}

\plitemsep 0.2in

\bci 

\item For Gaussian noise with $\mX = \trans{[\vx_1, \ldots, \vx_n]}$ and $\vy = \trans{[y_1, \ldots, y_n]},$
\aleq{
\vth_\ml  = \arg \min_{\vth} \frac{1}{2\sigma^2} \norm{\vy - \mX \vth}^2, \quad L(\vth) = \frac{1}{2\sigma^2} \norm{\vy - \mX \vth}^2
}

\item In case of Gaussian noise, $\vth_{\ml}= \vth$ that minimizes the empirical risk with the squared loss function
\bci
\item Models as functions $=$ Model as probabilistic models
\eci

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MLE (Maximum Likelihood Estimation) (3)}

\plitemsep 0.2in

\bci 

\item We find $\vth$ such that $\d{L}{\vth}=0$ 
\aleq{
&\d{L}{\vth} = \frac{1}{2\sigma^2} \left (-2 \trans{(\vy - \mX \vth)} \mX\right) = \frac{1}{\sigma^2} \left (-\trans{\vy}\vX +\trans{\vth}\trans{\mX}\mX  \right ) = 0\cr
&\Longleftrightarrow \trans{\vth}_\ml\trans{\mX}\mX = \trans{\vy}\vX \cr
& \Longleftrightarrow 
\trans{\vth}_\ml = \trans{\vy}\vX \inv{(\trans{\mX}\mX)} \quad \text{($\trans{\mX}\mX$ is positive definite if $\rk{\mX}=D$)} \cr
& \Longleftrightarrow \vth_\ml = \inv{(\trans{\mX}\mX)} \trans{\vX} \vy
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MLE with Features}

\plitemsep 0.07in

\bci 

\item Linear regression: Linear in terms of \bluef{the parameters}
\bci
\item $\trans{\phi(\vx)} \vth$ is also fine, where $\phi(\vx)$ can be non-linear (we will cover this later)
\item $\phi(\vx)$ are the features
\eci

\item Linear regression with the parameter $\vth \in \real^K,$ $\phi(\vx): \realD \mapsto \real^K$:
$$
p(y \mid \vx) = \set{N}(y \mid \trans{\phi(\vx)} \vth, \sigma^2) \Longleftrightarrow y = \trans{\phi(\vx)} \vth + \epsilon = \sum_{k=0}^{K-1} \theta_k \phi_k(\vx) + \epsilon
$$

\item \exam \bluef{Polynomial regression.} For $x\in \real$ and $\vth \in \real^K$, we lift the original 1-D input into $K$-D feature space with monomials $x^k$:
\aleq{
\phi(x) = \colvec{\phi_0(x) \\ \vdots \\ \phi_{K-1}(x)} = \colvec{1 \\ \vdots \\ x^{K-1}} \in \real^K
\quad \implies \quad f(x) = \sum_{k=0}^{K-1} \theta_k x^k
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Feature Matrix and MLE}

\plitemsep 0.15in

\bci 

\item Now, for the entire training set $\{\vx_1, \ldots, \vx_N \}$, 
\aleq{
\bm{\Phi} \eqdef \colvec{\trans{\phi}(\vx_1)\\ \vdots \\\trans{\phi}(\vx_N)}
= \begin{nmat}
\phi_0(\vx_1) & \cdots & \phi_{K-1}(\vx_1) \cr
\vdots & \cdots & \vdots \cr
\phi_0(\vx_N) & \cdots & \phi_{K-1}(\vx_N) 
\end{nmat}
\in \real^{N \times K}, \ \mPhi_{ij} = \phi_j(\vx_i), \ \phi_j: \realD \mapsto \real
}
\item Negative log-likelihood: Similarly to the case of $\vy = \mX \vth,$
\mycolorbox
{
\bci
\item $p(\set{Y}| \set{X},\vth) = \set{N}(\vy \mid \mPhi\vth, \sigma^2\mI)$
\item Negative-log likelihood for $f(\vx) = \trans{\phi}(\vx)\vth + \set{N}(0,\sigma^2)$: 
\vspace{-0.1cm}
$$
-\log p(\cY \mid \cX, \vth) = \dfrac{1}{2\sigma^2} \norm{\vy - \bm{\Phi}\vth}^2 + \text{const}
$$
\eci
}


% $$
% -\log p(\set{Y} \mid \set{X},\vth) = \dfrac{1}{2\sigma^2} \norm{\vy - \bm{\Phi}\vth}^2 + \text{const}
% $$

\item MLE:
$
\vth_\ml = \inv{(\trans{\bm{\Phi}}\bm{\Phi})} \trans{\bm{\Phi}} \vy
$
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Polynomial Fit}

\plitemsep 0.07in

\bci 

\item $N=10$ data, where $x_n \sim \set{U}[-5,5]$ and $y_n = -\sin(x_n/5) + \cos(x_n) + \epsilon,$ $\epsilon \sim \set{N}(0,0.2^2)$

\item Fit with poloynomial with degree 4 using ML
\eci

\mypic{0.8}{L9_poly4fit.png}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Overfitting in Linear Regression}

\myvartwocols{0.55}{0.7}{0.27}
{
\vspace{-0.4cm}
\mypic{0.95}{L9_overfit_linear.png}
}
{
%\vspace{-0.4cm}
\mypic{0.99}{L9_training_test.png}
}


\plitemsep 0.04in

\bci 

\item Higher polynomial degree is better (training error always decreases)

\item Test error increases after some polynomial degree
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(3)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item  \grayf{Problem Formulation}
\item  \grayf{Parameter Estimation: ML} 
\item  \redf{Parameter Estimation: MAP} 
\item  \grayf{Bayesian Linear Regression 
\item  Maximum Likelihood as Orthogonal Projection }

\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MAPE (Maximum A Posteriori Estimation)}

\plitemsep 0.15in

\bci 

\item MLE: prone to overfitting, where the magnitude of the parameters becomes large.
\item a prior distribution $p(\vth)$ helps: what $\vth$ is plausible 
\item MAPE and Bayes' theorem
\aleq{
p(\vth \mid \set{X},\set{Y}) = \frac{p(\set{Y} \mid \set{X}, \vth) p(\vth)}{p(\set{Y} \mid \set{X})}
\implies
\vth_\map  \in \arg\min_{\vth} \Big(-\log p(\set{Y} \mid \set{X},\vth) - \log p(\vth)\Big) 
}
\item Gradient
\aleq{
- \d{\log p(\vth | \set{X},\set{Y})}{\vth} = - \d{\log p(\set{Y}|\set{X},\vth)}{\vth} - \d{\log p(\vth)}{\vth}
}

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MAPE for Gausssian Prior (1)}

\plitemsep 0.07in

\bci 

\item \exam A (conjugate) Gaussian prior $p(\vth) \sim \set{N}(\vec{0}, b^2 \mI)$
\bci
\item For Gaussian likelihood, Gaussian prior $\implies$ Gaussian posterior \hfill \lecturemark{L6(6)}
\eci

\item Negative log-posterior

\medskip
\mycolorbox
{
Negative-log posterior for $f(\vx) = \trans{\phi}(\vx)\vth + \set{N}(0,\sigma^2)$ and $p(\vth) \sim \set{N}(\vec{0}, b^2 \mI)$:
\vspace{-0.1cm}
$$
-\log p(\vth | \set{X},\set{Y}) = \frac{1}{2\sigma^2} \trans{(\vy - \mPhi\vth)} (\vy - \mPhi\vth) + \frac{1}{2b^2}\trans{\vth}\vth + \text{const}
$$
}
\item Gradient
\aleq{
 -\d{\log p(\vth | \set{X},\set{Y})}{\vth} &= \frac{1}{\sigma^2} 
 (\trans{\vth}\trans{\mPhi}\mPhi - \trans{\vy}\mPhi) + \frac{1}{b^2}\trans{\vth}
}

\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MAPE for Gausssian Prior (2)}

\plitemsep 0.1in

\bci 

\item MAP vs. ML
$$
\vth_\map = \inv{
\underbrace{\Big(\trans{\mPhi} \mPhi + \bluef{\frac{\sigma^2}{b^2}\mI} \Big)}_{(*)}
} \trans{\mPhi} \vy, \quad \vth_\ml = \inv{(\trans{\bm{\Phi}}\bm{\Phi})} \trans{\bm{\Phi}} \vy
$$

\item The term $\bluef{\dfrac{\sigma^2}{b^2}\mI}$ 
\bci
\item Ensures that $(*)$ is symmetric, strictly positive definite
\item Role of regularizer
\eci
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Aside: MAPE for General Gausssian Prior (3)}

\plitemsep 0.07in

\bci 

\item \exam A (conjugate) Gaussian prior $p(\vth) \sim \bluef{\set{N}(\vm_0, \mS_0)}$

\item Negative log-posterior

\medskip

\begin{tcolorbox}[width=14cm,colback=red!5!white,colframe=red!75!black]
Negative-log posterior for $f(\vx) = \trans{\phi}(\vx)\vth + \set{N}(0,\sigma^2)$ and $p(\vth) \sim \set{N}(\vm_0, \mS_0)$:
\vspace{-0.1cm}
\aleq{
-\log p(\vth | \set{X},\set{Y}) &= \frac{1}{2\sigma^2} \trans{(\vy - \mPhi\vth)} (\vy - \mPhi\vth) + \bluef{\frac{1}{2}\trans{(\vth - \vm_0)}\inv{\mS}_0(\vth-\vm_0)}  + \text{const}
}
\end{tcolorbox}

\item We will use this later for computing the parameter posterior distribution in Bayesian linear regression.
\eci

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Regularization: MAPE vs. Explicit Regularizer}


\plitemsep 0.1in


\bci 

\item \bluef{Explicit regularizer} in regularized least squares (RLS)
$$
\norm{\vy - \mPhi\vth}^2 + \lambda \norm{\vth}^2
$$

\item \bluef{MAPE wth Gaussian prior} $p(\vth) \sim \set{N}(\vec{0},b^2 \mI)$
\bci
\item Negative log-Gaussian prior
$$
-\log p(\vth) = \frac{1}{2b^2}\trans{\vth}\vth + \text{const}
$$
\item $\lambda = 1/2b^2$ is the regularization term
\eci

\item Not surprising that we have
$$
\vth_{\text{RLS}} = \inv{
\Big(\trans{\mPhi} \mPhi + \bluef{\lambda \mI} \Big)
} \trans{\mPhi} \vy
$$
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(4)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item  \grayf{Problem Formulation}
\item  \grayf{Parameter Estimation: ML} 
\item  \grayf{Parameter Estimation: MAP} 
\item  \redf{Bayesian Linear Regression} 
\item  \grayf{Maximum Likelihood as Orthogonal Projection }

\ece
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bayesian Linear Regression}


\plitemsep 0.05in


\bci 

\item Earlier, ML and MAP. Now, \bluef{fully Bayesian} \hfill \lecturemark{L8(4)}

\item Model 
\mysmalltwocols{0.25}
{
\vspace{-0.2cm}
\aleq{
\text{prior} \quad & p(\vth) \sim \set{N}(\vm_0, \mS_0)   \cr
\text{likelihood} \quad& p(y | \vx, \vth) \sim \set{N}\big(y \mid \trans{\phi}(\vx)\vth,\sigma^2 \big)\cr
\text{joint} \quad & p(y,\vth | \vx) = p(y \mid \vx,\vth) p(\vth)
}
}
{
\vspace{-0.2cm}
\mypic{0.4}{L9_bayesian_regression.png}
}
\item Goal: For an input $\vx_*,$ we want to compute the following \bluef{posterior predictive distribution}\footnote{\lecturemark{Chapter 9.3.4} For ease of understanding, I've slightly changed the organization of these lecture slides from that of the textbook.} of $y_*$:
\vspace{-0.3cm}
$$
\displaystyle
 p(y_* | x_*, \set{X},\set{Y}) = \int \overbrace{p(y_* | \vx_*, \vth)}^{\text{likelihood}} 
 \overbrace{p(\vth | \set{X},\set{Y})}^{(*)}\text{d}\vth
$$
\vspace{-0.3cm}
\bci
\item $(*)$: parameter posterior distribution that needs to be computed
\eci
\eci

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Prior Predictions}


% \plitemsep 0.07in

% \bci 

% \item Fully Bayesian: Predictions by taking the parameter distribution and average over all plausible parameter setting. For an input $\vx_*$,
% $$
% p(y_* | \vx_*) = \int p(y,\vth | \vx) \text{d}\vth
% = \int p(y_* | \vx_*, \vth) p(\vth) \text{d}\vth = \bexpecti{\vth}{p(y_*|\vx_*, \vth)}
% $$

% \item This prediction based on \bluef{prior distribution} requires only input, not depending on the  training data. 

% \item Later, we will discuss \bluef{posterior prediction} which uses the modified predictive distribution based on the training data. 

% \item \bluef{Prior predictive distribution}
% $$
% \redf{p(y_* | x_*) = \set{N}\Big(\trans{\phi}(\vx_*)\vm_0, \trans{\phi}(\vx_*)\mS_0 \phi(\vx_*)+ 
% \sigma^2 \Big)}
% $$

% \eci

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Parameter Posterior Distribution (1)}

\plitemsep 0.07in

\bci 

\item \bluef{Parameter posterior distribution}\hfill \lecturemark{Chapter 9.3.3}
\mycolorbox{
$$
\redf{p(\vth \mid \set{X},\set{Y}) = \set{N}(\vth \mid \vm_N,\mS_N)}, \quad \text{where}  
$$
$$
\redf{
\mS_N = \inv{\big(\inv{\mS}_0 + \sigma^2 \trans{\mPhi}\mPhi \big)}, \quad \vm_N = \mS_N 
\big(\inv{\mS}_0\vm_0 + \sigma^{-2}\trans{\mPhi}\vy \big)}
$$
}
\eci
(Proof Sketch)
\small
\bci 

\item From the negative-log posterior for general Gaussian prior,
\aleq{
-\log p(\vth | \set{X},\set{Y}) &= \frac{1}{2\sigma^2} \trans{(\vy - \mPhi\vth)} (\vy - \mPhi\vth) + \bluef{\frac{1}{2}\trans{(\vth - \vm_0)}\inv{\mS}_0(\vth-\vm_0)}  + \text{const}
}

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Parameter Posterior Distribution (2)}

\plitemsep 0.07in
\bci 
\small
\item[] 
\aleq{
&= \frac{1}{2} \Big( 
\sigma^{-2}\trans{\vy}\vy - \orangef{2\sigma^{-2}\trans{\vy}\mPhi\vth} + \cyanf{\trans{\vth}\sigma^{-2} 
\trans{\mPhi}\mPhi\vth} + \cyanf{\trans{\vth}\inv{\mS}_0\vth} -\orangef{2\trans{\vm}_0\inv{\mS}_0\vth} 
+\trans{\vm}_0\inv{\mS}_0\vm_0
\Big ) \cr
&=\frac{1}{2} \Big( 
\cyanf{\trans{\vth}(\sigma^{-2}\trans{\mPhi}\mPhi + \inv{\mS}_0)\vth} 
-\orangef{2\trans{(\sigma^{-2}\trans{\mPhi}\vy + \inv{\mS}_0\vm_0)}\vth}
\Big) + \text{const}
}
\item \cyanf{cyan color}: quadratic term, \orangef{orange color}: linear term

\item $p(\vth|\cX,\cY) \propto \exp(\text{ quadratic in $\vth$ })$ $\implies$ Gaussian distribution
\item Assume that $p(\vth|\cX,\cY) = \set{N}(\vth|\vm_N, \mS_N),$ and find $\vm_N$ and $\mS_N.$
\aleq{
- \log\set{N}(\vth|\vm_N,\mS_N) &= \frac{1}{2}\trans{(\vth-\vm_N)}\inv{\mS}_N(\vth-\vm_N) + \text{const} \cr
&= \frac{1}{2}\Big( 
\cyanf{\trans{\vth}\inv{\mS}_N\vth} - \orangef{2\trans{\vm_N}\inv{\mS}_N\vth} + \trans{\vm}_N\inv{\mS}_N\vm_N  
\Big) + \text{const}
}
\item Thus, 
$
\cyanf{\inv{\mS}_N = \sigma^{-2}\trans{\mPhi}\mPhi + \inv{\mS}_0} \quad \text{and} \quad 
\orangef{
\trans{\vm}_N\inv{\mS}_N = \trans{(\sigma^{-2}\trans{\mPhi}\vy + \inv{\mS}_0\vm_0)}
}
$
\eci

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Posterior Predictions (1)}

\plitemsep 0.07in

\bci 

\item \bluef{Posterior predictive distribution} \hfill \lecturemark{L6(5)}
\red{
\aleq{
 p(y_* | x_*, \set{X},\set{Y}) &= \int p(y_* | \vx_*, \vth)p(\vth | \set{X},\set{Y})\text{d}\vth \cr
  &= \int \set{N}\Big(y_* | \trans{\phi}(\vx_*)\vth, \sigma^2\Big) \set{N}\Big(\vth | \vm_N, \mS_N\Big)\text{d}\vth \cr
  &= \set{N}\Big(y_* | \trans{\phi}(\vx_*)\vm_N, \trans{\phi}(\vx_*)\mS_N \phi(\vx_*)+ 
  \sigma^2 \Big)
}}

\item The mean $\trans{\phi}(\vx_*)\vm_N$ coincides with the MAP estimate
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Posterior Predictions (2)}

\mypic{0.95}{L9_posterior_predictive_ex.png}

\bci
\item BLR: Bayesian Linear Regression
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Computing Marginal Likelihood}

\plitemsep 0.07in

\bci 

\item Likelihood: $p(\cY | \cX, \vth),$ \bluef{Marginal likelihood}: $p(\cY | \cX) = \int p(\cY | \cX,\vth)p(\vth)\text{d}\vth$

\item Recall that the marginal likelihood is important for model selection via Bayes factor: 
$$
\text{(Posterior odds)} = \frac{\cprob{M_1 \mid \set{D}}}{\cprob{M_2 \mid \set{D}}} = \frac
{
\frac{\cprob{\set{D} \mid M_1}\cprob{M_1}}{\cprob{\set{D}}}
}
{
\frac{\cprob{\set{D} \mid M_2}\cprob{M_2}}{\cprob{\set{D}}}
}
= \underbrace{\frac{\cprob{M_1}}{\cprob{M_2}}}_{\text{Prior odds}} 
\underbrace{\frac{\cprob{\set{D} \mid M_1}}{\cprob{\set{D} \mid M_2}}}_{\bluef{\text{Bayes factor}}}
$$

\item[] 
\aleq{
p(\cY | \cX) &= \int p(\cY | \cX,\vth)p(\vth)\text{d}\vth = \int \set{N}(\vy | \mPhi\vth, \sigma^2\mI)
\set{N}(\vth | \vm_0,\mS_0)\, \text{d}\vth \cr
& = \set{N}(\vy \mid \mPhi\vm_0, \mPhi\mS_0\trans{\mPhi} + \sigma^2\mI)
}
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L9(5)}
\begin{frame}{Roadmap}

\plitemsep 0.1in

\bce[(1)] 

\item  \grayf{Problem Formulation}
\item  \grayf{Parameter Estimation: ML} 
\item  \grayf{Parameter Estimation: MAP} 
\item  \grayf{Bayesian Linear Regression} 
\item  \redf{Maximum Likelihood as Orthogonal Projection}

\ece
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{ML as Orthogonal Projection}

\plitemsep 0.07in

\bci 

\item For $f(\vx) = \trans{\vx}\vth + \set{N}(0,\sigma^2),$ $\vth_\ml = \inv{(\trans{\mX}\mX)} \trans{\vX} \vy = \dfrac{\trans{\vX}\vy}{\trans{\mX}\mX} \in \real$
$$
\mX\vth_\ml =  \dfrac{\vX\trans{\vX}}{\trans{\mX}\mX}\vy
$$
\vspace{-0.3cm}
\bci
\item Orthogonal projection of $\vy$ onto the one-dimensional subspace spanned by $\mX$
\eci


\item For $f(\vx) = \trans{\phi}(\vx)\vth + \set{N}(0,\sigma^2),$ $\vth_\ml = \inv{(\trans{\mPhi}\mPhi)} \trans{\mPhi} \vy = \dfrac{\trans{\mPhi}\vy}{\trans{\mPhi}\mPhi} \in \real$
$$
\mPhi\vth_\ml =  \dfrac{\mPhi\trans{\mPhi}}{\trans{\mPhi}\mPhi}\vy
$$
\vspace{-0.3cm}
\bci
\item Orthogonal projection of $\vy$ onto the $K$-dimensional subspace spanned by columns of $\mPhi$
\eci
\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Summary and Other Issues (1)}

\plitemsep 0.07in

\bci 

\item Linear regression for Gaussian likelihood and conjugate Gaussian priors. Nice analytical results and closed forms 

\item Other forms of likelihoods for other applications (e.g., classification)

\item GLM (generalized linear model): $y = \sigma \circ f$ ($\sigma$: activation function)
\bci
\item No longer linear in $\vth$
\item Logistic regression: $\sigma(f) = \dfrac{1}{1+\exp(-f)} \in [0,1]$ (interpreted as the probability of becoming 1)
\item Building blocks of (deep) feedforward neural nets

\item $\vy = \sigma(\mA \vx + \vb)$. $\mA$: weight matrix, $\vb$: bias vector
\item $K$-layer deep neural nets: $\vx_{k+1} = f_k(\vx_k),$ $f_k(\vx_k) = \sigma_k(\mA_k\vx_k + \vb_k)$
\eci

\eci
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Summary and Other Issues (2)}

\plitemsep 0.1in

\bci 

\item Gaussian process
\bci
\item A distribution over parameters $\rightarrow$ a distribution over functions
\item Gaussian process: distribution over functions without detouring via parameters
\item Closely related to BLR and support vector regression, also interpreted as Bayesian neural network with a single hidden layer and the infinite number of units
\eci

\item Gaussian likelihood, but non-Gaussian prior
\bci
\item When $N \ll D$ (small training data)
\item Prior that enforces sparsity, e.g., Laplace prior 
\item A linear regression with the Laplace prior $=$ linear regression with LASSO (L1 regularization)
\eci
\eci
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{}
\vspace{2cm}
\LARGE Questions?


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Review Questions}
% \tableofcontents
%\plitemsep 0.1in
\bce[1)]
\item 

\ece
\end{frame}


\end{document}
